---
title: "FinalProject"
author: "Rich Pihlstrom"
date: "2023-04-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(boot)
library(caret)
library(car)
library(ROCR)
```

# Linear Regression

## Upload Data
```{r}
df <- read.delim("bank.data.1.csv", sep = ",")
df <- df[,!names(df) %in% c("OTSREGNM_","SPECGRPN_")]
```

## Create Levels For Categorical Variables

### Replacement Function

The function below takes the input vectors of column names and category names with binary encoding and returns a single vector of category values.
```{r}
level <- function(NAME, colNames, catNames){
  
  colList <- c()
  removeList <- c()
  
  for(i in seq(1,nrow(df))){
    j <- 1
    for(name in colNames){
      if(df[i,name] == 1){
        colList <- append(colList, catNames[j])
        break
      }
      if(j == length(colNames)){
        removeList <- append(removeList, i)
      }
      j <- j+1
    }
  }
  
  if(length(removeList)>0){
    df <- df[-removeList,]
  }
  df <- df[,!names(df) %in% colNames]
  df[NAME] <- colList
  
  return(df)
  
}
```

### Running Function

The following sets of columns correlate to the binary one-hot encoding of a categorical variable from the original dataset:

* OTSREGNM:
  * OTSREGNM_Central
  * OTSREGNM_Northeast
  * OTSREGNM_Southeast
  * OTSREGNM_West
  * OTSREGNM_Western
* REGAGNT
  * REGAGNT_FDIC
  * REGAGNT_FED
  * REGAGNT_OCC
* SPECGRPN
  * SPECGRPN_Agricultural.Specialization
  * SPECGRPN_All.Other.Over.1.Billion
  * SPECGRPN_All.Other.Under.1.Billion
  * SPECGRPN_Commercial.Lending.Specialization
  * SPECGRPN_Consumer.Lending.Specialization
  * SPECGRPN_Credit.card.Specialization
  * SPECGRPN_International.Specialization
  * SPECGRPN_Mortgage.Lending.Specialization
  * SPECGRPN_Other.Specialized.Under.1.Billion

```{r}
colnames(df)
```

#### OTSREGNM
```{r}
OTSREGNM_names <- colnames(df)[30:34]
OTSREGNM_cats <- c("central","northeast","southeast","west","western")
df <- level("OTSREGNM", OTSREGNM_names, OTSREGNM_cats)
```

#### REGAGNT
```{r}
REGAGNT_names <- colnames(df)[30:32]
REGAGNT_cats <- c("FDIC","FED","OCC")
df <- level("REGAGNT", REGAGNT_names, REGAGNT_cats)
```

#### SPECGRPN
```{r}
colnames(df)
SPECGRPN_names <- colnames(df)[30:38]
SPECGRPN_cats <- c("Agricultural.Specialization","All.Other.Over.1.Billion","All.Other.Under.1.Billion",
                   "Commercial.Lending.Specialization","Consumer.Lending.Specialization",
                   "Credit.card.Specialization","International.Specialization",
                   "Mortgage.Lending.Specialization","Other.Specialized.Under.1.Billion")
df <- level("SPECGRPN", SPECGRPN_names, SPECGRPN_cats)
```

Drop Index Column:
```{r}
df <- df[,-1]
```

## Linear Regression

### Mutate the categorical columns to be factors:
```{r}
df <- mutate(df, 
                 OTSREGNM=as.factor(OTSREGNM),
                 REGAGNT=as.factor(REGAGNT),
                 SPECGRPN=as.factor(SPECGRPN))
```

The baseline categories will be the alphabetically-first category for each variable:

* OTSREGNM: "central"
* REGAGNT: "FDIC"
* SPECGRPN: "Agricultural.Specialization"

### Fit the model and determine the optimal lambda value to use to get best model.
```{r}
x = model.matrix(UBPRD486 ~ ., data=df)
y = df$UBPRD486

library(glmnet)

set.seed(1)
cv_model <- cv.glmnet(x, y, alpha = 1, type.measure="auc")

best_lambda <- cv_model$lambda.1se
best_lambda

plot(cv_model)

best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)
```

### Using the listed coefficient with non-zero effect, we can identify the following variables as the ones selected for out model:

* UBPRE575
* UBPRE589
* UBPRM026
* UBPRE014
* UBPRE015
* UBPRE544
* UBPRE600
* SPECGRPN

### With these selected variables, we can fit the data to the following regression model using a 5-fold cross validation:

```{r}
set.seed(1)
sel.names <- c("UBPRE575","UBPRE589","UBPRM026","UBPRE014","UBPRE015","UBPRE544","UBPRE600","SPECGRPN")

fit.lasso <- train(as.formula(paste("UBPRD486",paste(sel.names, collapse = "+"),sep=" ~ ")), method = "lm",
                     trControl = trainControl(method="cv", number=5, savePredictions = TRUE),
                     data=df)

summary(fit.lasso$finalModel)
```

From our output above, at a confidence level of $\alpha= 0.9$ we obtain the following regression equation for our final model:

\begin{align*} 
&\text{UBPRD486}=15.371595 +0.078242*(UBPRE589) -0.025207*(UBPRM026) -0.045603*(UBPRE014)\\
&-0.060019*(UBPRE015)+ 0.436990*(UBPRE544) +0.062084*(UBPRE600) -0.782223*(SPECGRPN_{All.Other.Over.1.Billion})\\
&-1.056788*(SPECGRPN_{Commercial.Lending.Specialization})-1.381076*(SPECGRPN_{Consumer.Lending.Specialization})\\
&-3.591104*(SPECGRPN_{International.Specialization}) +1.517069*(SPECGRPN_{Mortgage.Lending.Specialization})\\
&+2.591061*(SPECGRPN_{Other.Specialized.Under.1.Billion }) 
\end{align*} 

### We can then examine the predictive quality of our final model, both full and cross-validated:

Full model:
```{r}
predfin.lm <- predict(fit.lasso)
head(cbind(df$UBPRD486,predfin.lm))
plot(df$UBPRD486 ~ predfin.lm, xlab=expression(hat(UBPRD486)), ylab = "UBPRD486")
cor(df$UBPRD486, predfin.lm)
```

Cross-validated:
```{r}
pred.lm <- fit.lasso$pred
head(cbind(df$UBPRD486[pred.lm$rowIndex],pred.lm$pred))
plot(df$UBPRD486[pred.lm$rowIndex] ~ pred.lm$pred, xlab=expression(hat(UBPRD486.CV)), ylab = "UBPRD486")
cor(df$UBPRD486[pred.lm$rowIndex], pred.lm$pred)
```

In both cases, our correlation is not very strong â€” $<0.5$.

### We can now check to see if our final model has any multicollinearity problems:
```{r}
vif(fit.lasso$finalModel)
```

We see that no values surpass the VIF threshold so there is no multicollinearity issue.

### Model Assumptions

Independence:
The banks sampled for this dataset are almost certainly not independent, as they likely have significant influence over other banks' policies, resulting in influenced data points.

Linearity:
```{r}
par(mfrow = c(1, 2))
plot(as.formula(paste("UBPRD486",paste(sel.names, collapse = "+"),sep=" ~ ")), data=df)
```

None of the above scatterplots indicate linearity between our covariates and our dependent variable. This lack of support seems to moreso be a result of outliers than a different relationship like logarithmic.

Constant variance: 
```{r}
ncvTest(fit.lasso$finalModel)
```
We reject the null hypothesis of homoscedasticity with the BP test below, indicating that variance is non-constant.

Normality: 
```{r}
shapiro.test(rstandard(fit.lasso$finalModel))
```
We reject the null hypothesis and conclude that there is evidence that the residuals depart from normality.

#### Remove outliers

Visualize the studentized residuals that are outside of the range of [-3,3]
```{r}
plot(rstandard(fit.lasso$finalModel) ~ fit.lasso$finalModel$fitted.values,
     xlab="fitted values", ylab="Standardized residuals")
abline(0,0, lty=2)
abline(-3, 0, lty=2, col="red")
abline(3, 0, lty=2, col="red")
```

Remove the points outside of those bounds:
```{r}
i <- 1
indices <- c()
for(val in rstandard(fit.lasso$finalModel)){
  if(val > 3 || val < -3){
    indices <- append(indices, i)
  }
  i <- i + 1
}

df.OL <- df[-indices,]

```

Refit the cross-validated model with the removed outliers:
```{r}
fit.lasso.outlier <- train(as.formula(paste("UBPRD486",paste(sel.names, collapse = "+"),sep=" ~ ")), 
                           method = "lm",
                           trControl = trainControl(method="cv", number=5, savePredictions = TRUE),
                           data=df.OL)

summary(fit.lasso$finalModel)
```

Check the predictive quality:
```{r}
predfin.lm <- predict(fit.lasso.outlier)
head(cbind(df.OL$UBPRD486,predfin.lm))
plot(df.OL$UBPRD486 ~ predfin.lm, xlab=expression(hat(UBPRD486)), ylab = "UBPRD486")
cor(df.OL$UBPRD486, predfin.lm)
```

The scatter plot seems to be more coorelated, but the correlation is significantly worse than the previous model.




